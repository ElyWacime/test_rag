Ollama allows you to run large language models locally. It can be used as the LLM backend for LangChain.
